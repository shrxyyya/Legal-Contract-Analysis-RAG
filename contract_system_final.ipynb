{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 477520,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 384031,
          "modelId": 403438
        }
      ],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio pdfplumber rank-bm25 pinecone huggingface_hub"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-19T07:49:23.703496Z",
          "iopub.execute_input": "2025-07-19T07:49:23.703655Z",
          "iopub.status.idle": "2025-07-19T07:49:32.004941Z",
          "shell.execute_reply.started": "2025-07-19T07:49:23.703639Z",
          "shell.execute_reply": "2025-07-19T07:49:32.004039Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ORDf_BTdvNkK",
        "outputId": "8b3f3c98-fe8c-4e7c-cc06-2cd810dfefc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rank-bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting pinecone\n",
            "  Downloading pinecone-7.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.4)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2025.7.14)\n",
            "Collecting pinecone-plugin-assistant<2.0.0,>=1.6.0 (from pinecone)\n",
            "  Downloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Collecting packaging (from gradio)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading pinecone-7.3.0-py3-none-any.whl (587 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rank-bm25, pypdfium2, pinecone-plugin-interface, packaging, pinecone-plugin-assistant, pinecone, pdfminer.six, pdfplumber\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "Successfully installed packaging-24.2 pdfminer.six-20250506 pdfplumber-0.11.7 pinecone-7.3.0 pinecone-plugin-assistant-1.7.0 pinecone-plugin-interface-0.0.7 pypdfium2-4.30.1 rank-bm25-0.2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "packaging"
                ]
              },
              "id": "8e3d316fc830460d9bef1ad7e96a6d24"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pinecone import Pinecone\n",
        "from rank_bm25 import BM25Okapi\n",
        "import pickle\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "7daN2ayCvc_P"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "PINECONE_INDEX_NAME = \"legal-contract-search\"\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "METADATA_NAMESPACE = \"contract_metadata\"\n",
        "CHUNKS_NAMESPACE = \"contract_chunks\""
      ],
      "metadata": {
        "id": "usOfKEVOve5R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QueryType(Enum):\n",
        "    RISK_ANALYSIS = \"risk_analysis\"\n",
        "    CLAUSE_EXTRACTION = \"clause_extraction\"\n",
        "    COMPLIANCE_CHECK = \"compliance_check\"\n",
        "    COMPARISON = \"comparison\"\n",
        "    GENERAL_SEARCH = \"general_search\""
      ],
      "metadata": {
        "id": "UtnyFVrqvwtp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SearchResult:\n",
        "    def __init__(self, id: str, text: str, metadata: Dict[str, Any], score: float, search_type: str):\n",
        "        self.id = id\n",
        "        self.text = text\n",
        "        self.metadata = metadata\n",
        "        self.score = score\n",
        "        self.search_type = search_type\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"SearchResult(id='{self.id}', score={self.score:.3f}, type='{self.search_type}')\"\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"SearchResult: {self.metadata.get('filename', 'Unknown')} (Score: {self.score:.3f})\""
      ],
      "metadata": {
        "id": "jpkSSvLTv0Qq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridSearchEngine:\n",
        "    def __init__(self, pinecone_api_key: str, index_name: str, embedding_model: str):\n",
        "        self.pc = Pinecone(api_key=pinecone_api_key)\n",
        "        self.index = self.pc.Index(index_name)\n",
        "        self.model = SentenceTransformer(embedding_model)\n",
        "        self.bm25 = None\n",
        "        self.corpus_texts = []\n",
        "        self.corpus_metadata = []\n",
        "\n",
        "    def load_corpus_for_bm25(self, chunks_data: List[Dict[str, Any]]):\n",
        "        \"\"\"Load corpus for BM25 indexing\"\"\"\n",
        "        self.corpus_texts = [chunk['text'] for chunk in chunks_data]\n",
        "        self.corpus_metadata = [chunk['metadata'] for chunk in chunks_data]\n",
        "        tokenized_corpus = [text.lower().split() for text in self.corpus_texts]\n",
        "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "    def vector_search(self, query: str, namespace: str = CHUNKS_NAMESPACE,\n",
        "                     top_k: int = 50, metadata_filter: Dict[str, Any] = None) -> List[SearchResult]:\n",
        "        \"\"\"Perform vector similarity search with optional metadata filtering\"\"\"\n",
        "        try:\n",
        "            query_embedding = self.model.encode([query])[0]\n",
        "\n",
        "            query_params = {\n",
        "                \"vector\": query_embedding.tolist(),\n",
        "                \"top_k\": top_k,\n",
        "                \"namespace\": namespace,\n",
        "                \"include_metadata\": True\n",
        "            }\n",
        "\n",
        "            if metadata_filter:\n",
        "                query_params[\"filter\"] = metadata_filter\n",
        "\n",
        "            results = self.index.query(**query_params)\n",
        "\n",
        "            search_results = []\n",
        "            for match in results['matches']:\n",
        "                search_results.append(SearchResult(\n",
        "                    id=match['id'],\n",
        "                    text=match['metadata'].get('text', ''),\n",
        "                    metadata=match['metadata'],\n",
        "                    score=match['score'],\n",
        "                    search_type=\"vector\"\n",
        "                ))\n",
        "\n",
        "            return search_results\n",
        "        except Exception as e:\n",
        "            print(f\"Vector search error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def bm25_search(self, query: str, top_k: int = 50,\n",
        "                   filename_filter: str = None) -> List[SearchResult]:\n",
        "        \"\"\"Perform BM25 keyword search with optional filename filtering\"\"\"\n",
        "        if self.bm25 is None:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            tokenized_query = query.lower().split()\n",
        "            scores = self.bm25.get_scores(tokenized_query)\n",
        "\n",
        "            top_indices = np.argsort(scores)[::-1][:top_k*2]\n",
        "\n",
        "            search_results = []\n",
        "            for idx in top_indices:\n",
        "                if scores[idx] > 0:\n",
        "                    metadata = self.corpus_metadata[idx]\n",
        "\n",
        "                    if filename_filter and filename_filter.lower() not in metadata.get('filename', ''):\n",
        "                        continue\n",
        "\n",
        "                    search_results.append(SearchResult(\n",
        "                        id=f\"bm25_{idx}\",\n",
        "                        text=self.corpus_texts[idx],\n",
        "                        metadata=metadata,\n",
        "                        score=scores[idx],\n",
        "                        search_type=\"bm25\"\n",
        "                    ))\n",
        "\n",
        "                    if len(search_results) >= top_k:\n",
        "                        break\n",
        "\n",
        "            return search_results\n",
        "        except Exception as e:\n",
        "            print(f\"BM25 search error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def search_metadata_namespace(self, query: str, top_k: int = 10,\n",
        "                               metadata_filter: Dict[str, Any] = None) -> List[SearchResult]:\n",
        "        \"\"\"Search metadata namespace for quick factual queries\"\"\"\n",
        "        try:\n",
        "            query_embedding = self.model.encode([query])[0]\n",
        "\n",
        "            query_params = {\n",
        "                \"vector\": query_embedding.tolist(),\n",
        "                \"top_k\": top_k,\n",
        "                \"namespace\": METADATA_NAMESPACE,\n",
        "                \"include_metadata\": True\n",
        "            }\n",
        "\n",
        "            if metadata_filter:\n",
        "                query_params[\"filter\"] = metadata_filter\n",
        "\n",
        "            results = self.index.query(**query_params)\n",
        "\n",
        "            search_results = []\n",
        "            for match in results['matches']:\n",
        "                search_results.append(SearchResult(\n",
        "                    id=match['id'],\n",
        "                    text=match['metadata'].get('embedding_text', ''),\n",
        "                    metadata=match['metadata'],\n",
        "                    score=match['score'],\n",
        "                    search_type=\"metadata\"\n",
        "                ))\n",
        "\n",
        "            return search_results\n",
        "        except Exception as e:\n",
        "            print(f\"Metadata search error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def enhanced_hybrid_search(self, query: str, top_k: int = 20, alpha: float = 0.7,\n",
        "                  metadata_filter: Dict[str, Any] = None,\n",
        "                  filename_filter: str = None) -> List[SearchResult]:\n",
        "        \"\"\"Enhanced hybrid search that combines metadata and chunks information\"\"\"\n",
        "\n",
        "        # Get results from both namespaces\n",
        "        metadata_results = self.search_metadata_namespace(\n",
        "            query, top_k=top_k//2, metadata_filter=metadata_filter\n",
        "        )\n",
        "\n",
        "        vector_results = self.vector_search(query, top_k=top_k*2, metadata_filter=metadata_filter)\n",
        "        bm25_results = self.bm25_search(query, top_k=top_k*2, filename_filter=filename_filter)\n",
        "\n",
        "        # Normalize scores for vector and BM25 results\n",
        "        if vector_results:\n",
        "            max_vector_score = max(r.score for r in vector_results)\n",
        "            if max_vector_score > 0:\n",
        "                for result in vector_results:\n",
        "                    result.score = result.score / max_vector_score\n",
        "\n",
        "        # CORRECTED: Proper BM25 normalization with zero-division protection\n",
        "        if bm25_results:\n",
        "            bm25_scores = [r.score for r in bm25_results]\n",
        "            max_bm25_score = max(bm25_scores)\n",
        "            min_bm25_score = min(bm25_scores)\n",
        "\n",
        "            # Handle edge cases for normalization\n",
        "            if max_bm25_score > 0 and max_bm25_score != min_bm25_score:\n",
        "                # Use min-max normalization to avoid zero division\n",
        "                score_range = max_bm25_score - min_bm25_score\n",
        "                for result in bm25_results:\n",
        "                    result.score = (result.score - min_bm25_score) / score_range\n",
        "            elif max_bm25_score > 0:\n",
        "                # All scores are the same positive value, normalize to 1.0\n",
        "                for result in bm25_results:\n",
        "                    result.score = 1.0\n",
        "            else:\n",
        "                # All scores are zero or negative, set to small positive value\n",
        "                for result in bm25_results:\n",
        "                    result.score = 0.001\n",
        "\n",
        "        # Normalize metadata scores (they're already cosine similarity 0-1)\n",
        "        # Give metadata results a slight boost since they contain high-level information\n",
        "        for result in metadata_results:\n",
        "            result.score = result.score * 1.1  # 10% boost for metadata\n",
        "\n",
        "        # Combine all results\n",
        "        combined_results = {}\n",
        "\n",
        "        # Add metadata results first (they provide high-level context)\n",
        "        for result in metadata_results:\n",
        "            text_key = result.text[:100] + result.metadata.get('filename', '')\n",
        "            combined_results[text_key] = result\n",
        "            result.search_type = \"metadata\"\n",
        "\n",
        "        # Add vector results\n",
        "        for result in vector_results:\n",
        "            text_key = result.text[:100] + result.metadata.get('filename', '')\n",
        "            if text_key not in combined_results:  # Don't override metadata results\n",
        "                combined_results[text_key] = result\n",
        "                result.score = alpha * result.score\n",
        "\n",
        "        # Add BM25 results\n",
        "        for result in bm25_results:\n",
        "            text_key = result.text[:100] + result.metadata.get('filename', '')\n",
        "            if text_key in combined_results:\n",
        "                # Boost existing results with BM25 score\n",
        "                combined_results[text_key].score += (1 - alpha) * result.score\n",
        "                if combined_results[text_key].search_type == \"vector\":\n",
        "                    combined_results[text_key].search_type = \"hybrid\"\n",
        "                elif combined_results[text_key].search_type == \"metadata\":\n",
        "                    combined_results[text_key].search_type = \"metadata+bm25\"\n",
        "            else:\n",
        "                result.score = (1 - alpha) * result.score\n",
        "                combined_results[text_key] = result\n",
        "\n",
        "        # CORRECTED: Handle empty results case\n",
        "        if not combined_results:\n",
        "            print(f\"Warning: No results found for query: {query}\")\n",
        "            return []\n",
        "\n",
        "        final_results = sorted(combined_results.values(), key=lambda x: x.score, reverse=True)\n",
        "        return final_results[:top_k]"
      ],
      "metadata": {
        "id": "fDkeL-40v6pm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QueryClassifier:\n",
        "    def __init__(self):\n",
        "        self.risk_keywords = ['risk', 'liability', 'penalty', 'breach', 'default', 'damages', 'indemnification', 'exposure', 'consequence']\n",
        "        self.clause_keywords = ['clause', 'term', 'provision', 'section', 'article', 'condition', 'requirement', 'obligation']\n",
        "        self.compliance_keywords = ['compliance', 'regulation', 'law', 'legal', 'regulatory', 'standard', 'requirement', 'audit']\n",
        "        self.comparison_keywords = ['compare', 'difference', 'similar', 'contrast', 'versus', 'vs', 'between', 'against']\n",
        "\n",
        "        # Enhanced indicators with more precise patterns\n",
        "        self.general_indicators = [\n",
        "            'typical', 'common', 'usual', 'standard', 'general', 'normally', 'generally',\n",
        "            'what are', 'what should', 'how to', 'best practices', 'types of', 'kinds of',\n",
        "            'explain', 'describe', 'define', 'what is', 'how does', 'why do'\n",
        "        ]\n",
        "\n",
        "        # More specific contract indicators\n",
        "        self.specific_indicators = [\n",
        "            'in the', 'from the', 'show me', 'extract from', 'analyze the', 'review the',\n",
        "            'find in', 'locate in', 'summarize the', 'summarise the', 'according to',\n",
        "            'based on the', 'as per the'\n",
        "        ]\n",
        "\n",
        "    def classify_query(self, query: str) -> QueryType:\n",
        "        \"\"\"Classify the query type based on keywords\"\"\"\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        if any(keyword in query_lower for keyword in self.risk_keywords):\n",
        "            return QueryType.RISK_ANALYSIS\n",
        "        elif any(keyword in query_lower for keyword in self.clause_keywords):\n",
        "            return QueryType.CLAUSE_EXTRACTION\n",
        "        elif any(keyword in query_lower for keyword in self.compliance_keywords):\n",
        "            return QueryType.COMPLIANCE_CHECK\n",
        "        elif any(keyword in query_lower for keyword in self.comparison_keywords):\n",
        "            return QueryType.COMPARISON\n",
        "        else:\n",
        "            return QueryType.GENERAL_SEARCH\n",
        "\n",
        "    def extract_contract_identifier(self, query: str) -> Optional[str]:\n",
        "        \"\"\"Enhanced contract identifier extraction with better filtering\"\"\"\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        # Check for specific indicators first\n",
        "        has_specific_indicator = any(indicator in query_lower for indicator in self.specific_indicators)\n",
        "\n",
        "        if not has_specific_indicator:\n",
        "            return None\n",
        "\n",
        "        # Enhanced exclusion of general terms\n",
        "        general_terms = [\n",
        "            'typical', 'common', 'standard', 'general', 'what are', 'what should',\n",
        "            'types of', 'kinds of', 'best', 'worst', 'most', 'least', 'any',\n",
        "            'all', 'some', 'many', 'few', 'several', 'various'\n",
        "        ]\n",
        "        if any(term in query_lower for term in general_terms):\n",
        "            return None\n",
        "\n",
        "        # Improved contract patterns with more specific capture\n",
        "        contract_patterns = [\n",
        "            # Direct contract mentions with action words\n",
        "            r'(?:summarise|summarize|analyze|review|examine|check)\\s+(?:the\\s+)?([a-zA-Z0-9_-]+(?:\\s+[a-zA-Z0-9_-]+)*)\\s+(?:contract|agreement|document)',\n",
        "\n",
        "            # Prepositional phrases\n",
        "            r'(?:in|from|of|within)\\s+(?:the\\s+)?([a-zA-Z0-9_-]+(?:\\s+[a-zA-Z0-9_-]+)*)\\s+(?:contract|agreement)',\n",
        "\n",
        "            # Contract name patterns\n",
        "            r'\\b([a-zA-Z0-9_-]+(?:\\s+[a-zA-Z0-9_-]+)*)\\s+(?:contract|agreement)(?:\\s+(?:file|document))?\\b',\n",
        "\n",
        "            # Named contracts\n",
        "            r'(?:contract|agreement)\\s+(?:called|named|titled|for)\\s+([a-zA-Z0-9_-]+(?:\\s+[a-zA-Z0-9_-]+)*)',\n",
        "        ]\n",
        "\n",
        "        for pattern in contract_patterns:\n",
        "            matches = re.findall(pattern, query_lower, re.IGNORECASE)\n",
        "            if matches:\n",
        "                potential_contract = matches[0].strip()\n",
        "\n",
        "                # Enhanced filtering\n",
        "                excluded_words = {\n",
        "                    'the', 'this', 'that', 'a', 'an', 'contract', 'agreement', 'service',\n",
        "                    'typical', 'common', 'standard', 'general', 'main', 'key', 'important',\n",
        "                    'new', 'old', 'current', 'existing', 'draft', 'final', 'signed',\n",
        "                    'any', 'all', 'some', 'every', 'each', 'most', 'best', 'worst',\n",
        "                    'employment', 'lease', 'franchise', 'license', 'nda', 'merger'\n",
        "                }\n",
        "\n",
        "                # Clean and validate\n",
        "                contract_parts = potential_contract.split()\n",
        "                clean_parts = [part for part in contract_parts if part not in excluded_words and len(part) > 1]\n",
        "\n",
        "                if clean_parts and len(''.join(clean_parts)) > 2:\n",
        "                    return '_'.join(clean_parts).lower()\n",
        "\n",
        "        return None\n",
        "\n",
        "    def is_general_query(self, query: str) -> bool:\n",
        "        \"\"\"Check if query is asking for general information rather than specific contract analysis\"\"\"\n",
        "        query_lower = query.lower()\n",
        "        return any(indicator in query_lower for indicator in self.general_indicators)"
      ],
      "metadata": {
        "id": "qXXQh-pov_J6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContractAgent:\n",
        "    def __init__(self, agent_type: QueryType, model, tokenizer, device):\n",
        "        self.agent_type = agent_type\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "\n",
        "        self.system_prompts = {\n",
        "            QueryType.RISK_ANALYSIS: \"\"\"You are a legal risk analysis expert. Analyze the provided contract chunks and identify potential risks, liabilities, and areas of concern. Focus on:\n",
        "- Financial risks and liability exposure\n",
        "- Compliance risks\n",
        "- Operational risks\n",
        "- Legal risks\n",
        "- Confidentiality and Data privacy risks\n",
        "Provide a structured analysis with specific references to contract terms.\"\"\",\n",
        "\n",
        "            QueryType.CLAUSE_EXTRACTION: \"\"\"You are a contract clause extraction specialist. Extract and analyze specific clauses from the provided contract chunks. Focus on:\n",
        "- Key terms and conditions\n",
        "- Rights and obligations\n",
        "- Performance requirements\n",
        "- Termination clauses\n",
        "Present the information in a clear, structured format.\"\"\",\n",
        "\n",
        "            QueryType.COMPLIANCE_CHECK: \"\"\"You are a compliance verification expert. Review the provided contract chunks for compliance with legal and regulatory requirements. Focus on:\n",
        "- Regulatory compliance\n",
        "- Industry standards\n",
        "- Legal requirements\n",
        "- Best practices\n",
        "Highlight any compliance gaps or concerns.\"\"\",\n",
        "\n",
        "            QueryType.COMPARISON: \"\"\"You are a contract comparison specialist. Compare and contrast the provided contract chunks to identify:\n",
        "- Similarities and differences\n",
        "- Variations in terms\n",
        "- Inconsistencies\n",
        "- Relative advantages/disadvantages\n",
        "Provide a detailed comparative analysis.\"\"\",\n",
        "\n",
        "            QueryType.GENERAL_SEARCH: \"\"\"You are a general contract analysis expert. Analyze the provided contract chunks and provide relevant insights based on the user's query. Be comprehensive and accurate in your analysis.\"\"\"\n",
        "        }\n",
        "\n",
        "    def generate_response(self, query: str, search_results: List[SearchResult],\n",
        "             context_type: str = \"database\", min_token: int = 512, max_tokens: int = 2048) -> str:\n",
        "        \"\"\"Enhanced response generation with better context organization\"\"\"\n",
        "        try:\n",
        "            if context_type == \"uploaded\":\n",
        "                # Enhanced context for uploaded documents\n",
        "                doc_text = search_results[0].text\n",
        "                filename = search_results[0].metadata.get('filename', 'Unknown')\n",
        "\n",
        "                # Extract key sections if document is long\n",
        "                if len(doc_text) > 3000:\n",
        "                    sentences = doc_text.split('. ')\n",
        "                    # Use query keywords to find most relevant sections\n",
        "                    query_words = set(query.lower().split())\n",
        "                    scored_sentences = []\n",
        "\n",
        "                    for i, sentence in enumerate(sentences):\n",
        "                        sentence_words = set(sentence.lower().split())\n",
        "                        overlap = len(query_words.intersection(sentence_words))\n",
        "                        if overlap > 0:\n",
        "                            scored_sentences.append((overlap, i, sentence))\n",
        "\n",
        "                    # Sort by relevance and take top sections\n",
        "                    scored_sentences.sort(reverse=True)\n",
        "                    relevant_text = '. '.join([sent[2] for sent in scored_sentences[:10]])\n",
        "                    context = f\"Document: {filename}\\n\\nMost Relevant Sections:\\n{relevant_text}\"\n",
        "                else:\n",
        "                    context = f\"Document: {filename}\\n\\n{doc_text}\"\n",
        "\n",
        "            else:\n",
        "                # Enhanced database context organization\n",
        "                metadata_results = [r for r in search_results if r.search_type in [\"metadata\", \"metadata+bm25\"]]\n",
        "                chunk_results = [r for r in search_results if r.search_type not in [\"metadata\", \"metadata+bm25\"]]\n",
        "\n",
        "                context_parts = []\n",
        "\n",
        "                # Add metadata overview\n",
        "                if metadata_results:\n",
        "                    context_parts.append(\"=== CONTRACT OVERVIEW ===\")\n",
        "                    for result in metadata_results[:2]:\n",
        "                        context_parts.append(f\"Document: {result.metadata.get('filename', 'Unknown')}\")\n",
        "                        context_parts.append(f\"Summary: {result.text[:1000]}\")\n",
        "                        context_parts.append(\"\")\n",
        "\n",
        "                # Group chunks by document for better organization\n",
        "                if chunk_results:\n",
        "                    context_parts.append(\"=== DETAILED CONTENT ===\")\n",
        "\n",
        "                    # Group by filename\n",
        "                    doc_chunks = {}\n",
        "                    for result in chunk_results:\n",
        "                        filename = result.metadata.get('filename', 'Unknown')\n",
        "                        if filename not in doc_chunks:\n",
        "                            doc_chunks[filename] = []\n",
        "                        doc_chunks[filename].append(result)\n",
        "\n",
        "                    # Add chunks grouped by document\n",
        "                    for filename, chunks in list(doc_chunks.items())[:3]:  # Max 3 documents\n",
        "                        context_parts.append(f\"--- {filename.upper()} ---\")\n",
        "                        for i, chunk in enumerate(chunks[:2]):  # Max 2 chunks per doc\n",
        "                            context_parts.append(f\"Section {i+1}:\")\n",
        "                            context_parts.append(chunk.text[:1500])\n",
        "                            context_parts.append(\"\")\n",
        "\n",
        "                context = \"\\n\".join(context_parts)\n",
        "\n",
        "            # Enhanced system prompt based on query type\n",
        "            system_prompt = self.system_prompts.get(self.agent_type, \"\")\n",
        "\n",
        "            # Build comprehensive prompt\n",
        "            user_prompt = f\"\"\"System: {system_prompt}\n",
        "\n",
        "Query: {query}\n",
        "\n",
        "Contract Information:\n",
        "{context}\n",
        "\n",
        "Instructions:\n",
        "- Provide a detailed, accurate response based on the contract information\n",
        "- Quote specific clauses or sections when relevant\n",
        "- Explain legal implications clearly\n",
        "- Structure your response with clear headings if appropriate\n",
        "- If information is insufficient, state what additional details would be helpful\n",
        "\n",
        "Response:\"\"\"\n",
        "\n",
        "            # Generate with improved parameters\n",
        "            inputs = self.tokenizer(user_prompt, return_tensors=\"pt\", truncation=True, max_length=3072)\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=max_tokens,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.2,  # Lower temperature for more focused responses\n",
        "                    top_p=0.9,  # Add top_p sampling\n",
        "                    pad_token_id=self.tokenizer.eos_token_id,\n",
        "                    repetition_penalty=1.15,  # Slightly higher to reduce repetition\n",
        "                    no_repeat_ngram_size=3,\n",
        "                    early_stopping=True\n",
        "                )\n",
        "\n",
        "            # Enhanced response cleaning\n",
        "            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # Extract generated part\n",
        "            if \"Response:\" in full_response:\n",
        "                response = full_response.split(\"Response:\")[-1].strip()\n",
        "            else:\n",
        "                response = full_response[len(user_prompt):].strip()\n",
        "\n",
        "            # Clean and format response\n",
        "            response = self._clean_response(response)\n",
        "\n",
        "            return response if response else \"I apologize, but I couldn't generate a proper response. Please try rephrasing your query.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Generation error: {e}\")\n",
        "            return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "    def _clean_response(self, response: str) -> str:\n",
        "        \"\"\"Clean and format the generated response\"\"\"\n",
        "        lines = response.split('\\n')\n",
        "        cleaned_lines = []\n",
        "        prev_line = \"\"\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            # Remove repetitive lines and empty lines at the start\n",
        "            if line and line != prev_line and not (line.startswith(\"System:\") or line.startswith(\"Query:\")):\n",
        "                cleaned_lines.append(line)\n",
        "                prev_line = line\n",
        "\n",
        "        # Join and remove any remaining artifacts\n",
        "        final_response = '\\n'.join(cleaned_lines)\n",
        "\n",
        "        # Remove common generation artifacts\n",
        "        artifacts_to_remove = [\n",
        "            \"Based on the contract information provided:\",\n",
        "            \"According to the contract:\",\n",
        "            \"Here is my analysis:\",\n",
        "            \"Response:\",\n",
        "            \"Answer:\"\n",
        "        ]\n",
        "\n",
        "        for artifact in artifacts_to_remove:\n",
        "            if final_response.startswith(artifact):\n",
        "                final_response = final_response[len(artifact):].strip()\n",
        "\n",
        "        return final_response"
      ],
      "metadata": {
        "id": "lpziSoPYwJlq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnifiedContractSystem:\n",
        "    def __init__(self, pinecone_api_key: str, index_name: str, embedding_model: str):\n",
        "        self.search_engine = HybridSearchEngine(pinecone_api_key, index_name, embedding_model)\n",
        "        self.query_classifier = QueryClassifier()\n",
        "\n",
        "        # Initialize LLM\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
        "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "\n",
        "            # Initialize agents\n",
        "            self.agents = {\n",
        "                query_type: ContractAgent(query_type, self.model, self.tokenizer, self.device)\n",
        "                for query_type in QueryType\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing LLM: {e}\")\n",
        "            self.model = None\n",
        "            self.tokenizer = None\n",
        "            self.agents = {}\n",
        "\n",
        "        self.current_document = None\n",
        "        self.rag_initialized = False\n",
        "\n",
        "    def initialize_rag(self, chunks_data: List[Dict[str, Any]]):\n",
        "        \"\"\"Initialize RAG system with dataset\"\"\"\n",
        "        try:\n",
        "            self.search_engine.load_corpus_for_bm25(chunks_data)\n",
        "            self.rag_initialized = True\n",
        "            print(\"RAG system initialized successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing RAG: {str(e)}\")\n",
        "            self.rag_initialized = False\n",
        "\n",
        "    def load_document(self, document_text: str, filename: str = \"uploaded_contract\"):\n",
        "        \"\"\"Load the uploaded document for analysis\"\"\"\n",
        "        self.current_document = {\n",
        "            \"text\": document_text,\n",
        "            \"filename\": filename.lower()  # Ensure filename is lowered to match metadata\n",
        "        }\n",
        "\n",
        "    def determine_query_mode(self, query: str, has_uploaded_doc: bool) -> Tuple[str, Optional[str]]:\n",
        "        \"\"\"Determine whether to use uploaded document, RAG, or specific contract search\"\"\"\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        # Check if this is a general query first\n",
        "        is_general = self.query_classifier.is_general_query(query)\n",
        "\n",
        "        # Check for specific contract identifier\n",
        "        contract_id = self.query_classifier.extract_contract_identifier(query)\n",
        "\n",
        "        # Priority order:\n",
        "        # 1. If user uploaded a document and query is about \"this contract\" or \"the contract\"\n",
        "        if has_uploaded_doc and any(phrase in query_lower for phrase in ['this contract', 'uploaded contract', 'the contract', 'this document']):\n",
        "            return \"uploaded\", None\n",
        "\n",
        "        # 2. If query is clearly general/typical, use RAG regardless of contract identifier\n",
        "        if is_general:\n",
        "            return \"rag\", None\n",
        "\n",
        "        # 3. If query mentions specific contract identifier and RAG is available\n",
        "        if contract_id and self.rag_initialized:\n",
        "            return \"specific_contract\", contract_id\n",
        "\n",
        "        # 4. If user uploaded a document and query doesn't have general indicators\n",
        "        if has_uploaded_doc and not is_general:\n",
        "            return \"uploaded\", None\n",
        "\n",
        "        # 5. Default to RAG for general questions\n",
        "        return \"rag\", None\n",
        "\n",
        "    def create_metadata_filter(self, contract_id: str) -> Dict[str, Any]:\n",
        "        \"\"\"Create metadata filter for Pinecone search with exact match on lowered filenames\"\"\"\n",
        "        # Since all filenames in metadata are lowered, use $eq for exact match\n",
        "        return {\"filename\": {\"$eq\": contract_id.lower()}}\n",
        "\n",
        "    def process_query(self, query: str, top_k: int = 15) -> Dict[str, Any]:\n",
        "        \"\"\"Process user query through the unified system\"\"\"\n",
        "        if not query or query.strip() == \"\":\n",
        "            return {\n",
        "                \"query\": query,\n",
        "                \"query_type\": \"error\",\n",
        "                \"query_mode\": \"error\",\n",
        "                \"response\": \"Please enter a valid query.\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "\n",
        "        has_uploaded_doc = self.current_document is not None\n",
        "        query_mode, contract_id = self.determine_query_mode(query, has_uploaded_doc)\n",
        "        query_type = self.query_classifier.classify_query(query)\n",
        "\n",
        "        # Debug information\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"Query mode: {query_mode}\")\n",
        "        print(f\"Contract ID: {contract_id}\")\n",
        "        print(f\"Query type: {query_type}\")\n",
        "\n",
        "        # Check if LLM is available\n",
        "        if not self.model or not self.tokenizer:\n",
        "            return {\n",
        "                \"query\": query,\n",
        "                \"query_type\": \"error\",\n",
        "                \"query_mode\": \"error\",\n",
        "                \"response\": \"Language model not initialized. Cannot generate responses.\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            if query_mode == \"uploaded\":\n",
        "                # Process uploaded document\n",
        "                search_results = [SearchResult(\n",
        "                    id=\"uploaded_doc\",\n",
        "                    text=self.current_document[\"text\"],\n",
        "                    metadata={\"filename\": self.current_document[\"filename\"]},\n",
        "                    score=1.0,\n",
        "                    search_type=\"uploaded_document\"\n",
        "                )]\n",
        "\n",
        "                agent = self.agents[query_type]\n",
        "                response = agent.generate_response(query, search_results, context_type=\"uploaded\")\n",
        "\n",
        "                return {\n",
        "                    \"query\": query,\n",
        "                    \"query_type\": query_type.value,\n",
        "                    \"query_mode\": \"uploaded_document\",\n",
        "                    \"search_results\": 1,\n",
        "                    \"response\": response,\n",
        "                    \"sources\": [{\n",
        "                        \"filename\": self.current_document[\"filename\"],\n",
        "                        \"score\": 1.0,\n",
        "                        \"search_type\": \"uploaded_document\"\n",
        "                    }]\n",
        "                }\n",
        "\n",
        "            elif query_mode == \"specific_contract\":\n",
        "                # Search for specific contract in database\n",
        "                if not self.rag_initialized:\n",
        "                    return {\n",
        "                        \"query\": query,\n",
        "                        \"query_type\": query_type.value,\n",
        "                        \"query_mode\": \"error\",\n",
        "                        \"response\": \"RAG system not initialized. Cannot search contract database.\",\n",
        "                        \"sources\": []\n",
        "                    }\n",
        "\n",
        "                # Create metadata filter for both namespaces\n",
        "                metadata_filter = self.create_metadata_filter(contract_id)\n",
        "\n",
        "                # Use enhanced search that combines metadata and chunks\n",
        "                search_results = self.search_engine.enhanced_hybrid_search(\n",
        "                    query,\n",
        "                    top_k=top_k,\n",
        "                    metadata_filter=metadata_filter,\n",
        "                    filename_filter=contract_id.lower()\n",
        "                )\n",
        "\n",
        "                # CORRECTED: Handle empty results with fallback search\n",
        "                if not search_results:\n",
        "                    print(f\"No exact match for contract_id: {contract_id}. Attempting broader search with post-filtering.\")\n",
        "                    all_results = self.search_engine.enhanced_hybrid_search(query, top_k=top_k*2)\n",
        "                    search_results = [\n",
        "                        result for result in all_results\n",
        "                        if re.search(contract_id.lower(), result.metadata.get('filename', ''), re.IGNORECASE)\n",
        "                    ][:top_k]\n",
        "\n",
        "                # CORRECTED: Better handling of no results case\n",
        "                if not search_results:\n",
        "                    return {\n",
        "                        \"query\": query,\n",
        "                        \"query_type\": query_type.value,\n",
        "                        \"query_mode\": \"specific_contract\",\n",
        "                        \"response\": f\"No documents found for contract '{contract_id}' in the database. Please check the contract name and try again, or try a more general query.\",\n",
        "                        \"sources\": [],\n",
        "                        \"search_results\": 0\n",
        "                    }\n",
        "\n",
        "                agent = self.agents[query_type]\n",
        "                response = agent.generate_response(query, search_results, context_type=\"database\")\n",
        "\n",
        "                return {\n",
        "                    \"query\": query,\n",
        "                    \"query_type\": query_type.value,\n",
        "                    \"query_mode\": \"specific_contract\",\n",
        "                    \"search_results\": len(search_results),\n",
        "                    \"response\": response,\n",
        "                    \"sources\": [{\n",
        "                        \"filename\": result.metadata.get('filename', 'Unknown'),\n",
        "                        \"score\": result.score,\n",
        "                        \"search_type\": result.search_type\n",
        "                    } for result in search_results[:5]]\n",
        "                }\n",
        "\n",
        "            else:  # RAG mode\n",
        "                if not self.rag_initialized:\n",
        "                    return {\n",
        "                        \"query\": query,\n",
        "                        \"query_type\": query_type.value,\n",
        "                        \"query_mode\": \"error\",\n",
        "                        \"response\": \"RAG system not initialized. Cannot answer general questions.\",\n",
        "                        \"sources\": []\n",
        "                    }\n",
        "\n",
        "                # Use enhanced search that combines metadata and chunks for comprehensive results\n",
        "                search_results = self.search_engine.enhanced_hybrid_search(query, top_k=top_k)\n",
        "\n",
        "                # CORRECTED: Better handling of no results in RAG mode\n",
        "                if not search_results:\n",
        "                    return {\n",
        "                        \"query\": query,\n",
        "                        \"query_type\": query_type.value,\n",
        "                        \"query_mode\": \"rag\",\n",
        "                        \"response\": \"No relevant documents found in the database. Please try rephrasing your query or using different keywords.\",\n",
        "                        \"sources\": [],\n",
        "                        \"search_results\": 0\n",
        "                    }\n",
        "\n",
        "                agent = self.agents[query_type]\n",
        "                response = agent.generate_response(query, search_results, context_type=\"database\")\n",
        "\n",
        "                return {\n",
        "                    \"query\": query,\n",
        "                    \"query_type\": query_type.value,\n",
        "                    \"query_mode\": \"rag\",\n",
        "                    \"search_results\": len(search_results),\n",
        "                    \"response\": response,\n",
        "                    \"sources\": [{\n",
        "                        \"filename\": result.metadata.get('filename', 'Unknown'),\n",
        "                        \"score\": result.score,\n",
        "                        \"search_type\": result.search_type\n",
        "                    } for result in search_results[:5]]\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in process_query: {e}\")  # Added logging\n",
        "            return {\n",
        "                \"query\": query,\n",
        "                \"query_type\": \"error\",\n",
        "                \"query_mode\": \"error\",\n",
        "                \"response\": f\"Error processing query: {str(e)}\",\n",
        "                \"sources\": [],\n",
        "                \"search_results\": 0\n",
        "            }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-19T07:49:35.028462Z",
          "iopub.execute_input": "2025-07-19T07:49:35.029068Z",
          "iopub.status.idle": "2025-07-19T07:50:38.499071Z",
          "shell.execute_reply.started": "2025-07-19T07:49:35.029034Z",
          "shell.execute_reply": "2025-07-19T07:50:38.498280Z"
        },
        "collapsed": true,
        "id": "v5Xrob_yvNkO"
      },
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio Interface\n",
        "import gradio as gr\n",
        "import pdfplumber\n",
        "import tempfile\n",
        "\n",
        "def extract_text_from_file(file_path: str) -> str:\n",
        "    \"\"\"Extract text from uploaded txt or pdf file.\"\"\"\n",
        "    try:\n",
        "        if file_path.endswith(\".txt\"):\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                return f.read()\n",
        "        elif file_path.endswith(\".pdf\"):\n",
        "            with pdfplumber.open(file_path) as pdf:\n",
        "                return \"\\n\".join([page.extract_text() or \"\" for page in pdf.pages])\n",
        "        else:\n",
        "            return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Initialize system globally\n",
        "system = UnifiedContractSystem(\n",
        "    pinecone_api_key=os.environ.get(\"PINECONE_API_KEY\", \"pcsk_5mdDMR_HB4yT8PAsi5THNDFUZrpoiRHm68NKU6CmLaNj4AqmN46MtdSZM3h8TanPGVnZtk\"),\n",
        "    index_name=PINECONE_INDEX_NAME,\n",
        "    embedding_model=EMBEDDING_MODEL\n",
        ")\n",
        "\n",
        "# Try to initialize RAG system\n",
        "try:\n",
        "    chunks_with_metadata = pickle.load(open('/content/chunks_with_metadata.pkl', 'rb'))\n",
        "    system.initialize_rag(chunks_with_metadata)\n",
        "except FileNotFoundError:\n",
        "    print(\"Warning: chunks_with_metadata.pkl not found. RAG system not initialized.\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Error loading RAG data: {str(e)}\")\n",
        "\n",
        "def process_contract_query(file, query):\n",
        "    \"\"\"Process the uploaded file and query.\"\"\"\n",
        "    try:\n",
        "        if not query or query.strip() == \"\":\n",
        "            return \"Please enter a query.\", \"\", \"\"\n",
        "\n",
        "        # Handle file upload\n",
        "        if file is not None:\n",
        "            extracted_text = extract_text_from_file(file)\n",
        "            if not extracted_text.strip():\n",
        "                return \"No text could be extracted from the uploaded file.\", \"\", \"\"\n",
        "\n",
        "            filename = os.path.basename(file).lower() if file else \"uploaded_contract\"\n",
        "            system.load_document(extracted_text, filename)\n",
        "\n",
        "        # Process query\n",
        "        result = system.process_query(query)\n",
        "\n",
        "        response = result[\"response\"]\n",
        "\n",
        "        # Format sources\n",
        "        sources_text = \"\"\n",
        "        if result[\"sources\"]:\n",
        "            sources_text = f\"Query Mode: {result.get('query_mode', 'unknown')}\\n\"\n",
        "            sources_text += f\"Query Type: {result.get('query_type', 'unknown')}\\n\\n\"\n",
        "            sources_text += \"Sources:\\n\" + \"\\n\".join([\n",
        "                f\"• {src['filename']} (Score: {src['score']:.3f}, Type: {src['search_type']})\"\n",
        "                for src in result[\"sources\"]\n",
        "            ])\n",
        "\n",
        "        # Format metadata\n",
        "        metadata_text = f\"Found {result.get('search_results', 0)} relevant chunks\"\n",
        "\n",
        "        return response, sources_text, metadata_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error processing request: {str(e)}\", \"\", \"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qb5HeA9dwURv",
        "outputId": "0c71862f-efd1-441b-a3bb-003744d64c1b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG system initialized successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_interface():\n",
        "    with gr.Blocks(title=\"Unified Contract Analysis System\") as demo:\n",
        "        gr.Markdown(\"# 📋 Unified Contract Analysis System\")\n",
        "        gr.Markdown(\"\"\"\n",
        "        **Three modes of operation:**\n",
        "        - **Upload a document**: Analyze specific contracts you upload\n",
        "        - **Ask about specific contracts**: Query contracts in the database (e.g., \"What are the risks in acme contract?\")\n",
        "        - **General questions**: Ask general legal questions using our knowledge base\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                file_input = gr.File(\n",
        "                    label=\"Upload Contract Document (Optional)\",\n",
        "                    file_types=[\".txt\", \".pdf\"],\n",
        "                    type=\"filepath\"\n",
        "                )\n",
        "\n",
        "                query_input = gr.Textbox(\n",
        "                    label=\"Your Query\",\n",
        "                    placeholder=\"e.g., What are typical termination clauses? or What are the risks in acme contract?\",\n",
        "                    lines=3\n",
        "                )\n",
        "\n",
        "                submit_btn = gr.Button(\"Analyze\", variant=\"primary\")\n",
        "                clear_btn = gr.Button(\"Clear Document\", variant=\"secondary\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                response_output = gr.Textbox(\n",
        "                    label=\"AI Analysis\",\n",
        "                    lines=20,\n",
        "                    max_lines=30,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                sources_output = gr.Textbox(\n",
        "                    label=\"Sources & Metadata\",\n",
        "                    lines=10,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "                metadata_output = gr.Textbox(\n",
        "                    label=\"Search Info\",\n",
        "                    lines=3,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "        submit_btn.click(\n",
        "            fn=process_contract_query,\n",
        "            inputs=[file_input, query_input],\n",
        "            outputs=[response_output, sources_output, metadata_output]\n",
        "        )\n",
        "\n",
        "        def clear_document():\n",
        "            system.current_document = None\n",
        "            return None, \"\", \"\", \"\"\n",
        "\n",
        "        clear_btn.click(\n",
        "            fn=clear_document,\n",
        "            outputs=[file_input, response_output, sources_output, metadata_output]\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\"### Example Queries:\")\n",
        "        gr.Markdown(\"\"\"\n",
        "        **For uploaded documents:**\n",
        "        - \"What are the main risks in this contract?\"\n",
        "        - \"Extract all termination clauses from this document\"\n",
        "\n",
        "        **For specific contracts in database:**\n",
        "        - \"What are the parties involved in acme contract?\"\n",
        "        - \"Show me liability clauses in microsoft contract\"\n",
        "        - \"Analyze the adamsgolf contract\"\n",
        "\n",
        "        **General questions:**\n",
        "        - \"What are typical risks in service agreements?\"\n",
        "        - \"What are typical termination clauses?\"\n",
        "        - \"What should I look for in liability clauses?\"\n",
        "        \"\"\")\n",
        "\n",
        "    return demo"
      ],
      "metadata": {
        "id": "6olTQic6wiMX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the interface\n",
        "if __name__ == \"__main__\":\n",
        "    demo = create_interface()\n",
        "    demo.launch(share=True, show_error=True)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "2f_jQzjwvNkb",
        "outputId": "f90cad5f-c3c0-4884-f289-769c3410fe44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://363a924e52ceac2743.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://363a924e52ceac2743.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "xSVCWbj5vNkc"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}